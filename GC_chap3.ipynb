{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aa_lqIHks0zf"
      },
      "outputs": [],
      "source": [
        "#GAN implementation using keras and tensorflow\n",
        "\n",
        "#imports\n",
        "import numpy as np\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, BatchNormalization\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import plot_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(object):\n",
        "  def __init__(self, width=28, height=28, channels=1, latent_size=100):\n",
        "    #initialize variables\n",
        "    self.CAPACITY = width*height*channels\n",
        "    self.SHAPE = (width, height, channels)\n",
        "    self.OPTIMIZER = Adam(lr=0.0002, decay=8e-9)\n",
        "    self.Discriminator = self.model()\n",
        "\n",
        "    self.Discriminator.compile(loss='binary_crossentropy', optimizer=self.OPTIMIZER, metrics=['accuracy'])\n",
        "    self.Discriminator.summary\n",
        "\n",
        "  def model(self):\n",
        "    #build the binary classifier and return it\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=self.SHAPE))\n",
        "    model.add(Dense(self.CAPACITY, input_shape=self.SHAPE))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    model.add(Dense(int(self.CAPACITY/2)))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "  def summary(self):\n",
        "    #Prints the model summary to the screen\n",
        "    return self.Discriminator.summary()\n",
        "\n",
        "  def save_mode(self):\n",
        "    #saves the model structure to a file in the data folder\n",
        "    plot_model(self.Discriminator.model, to_file='/content/GAN_100/Discriminator_model.png')\n"
      ],
      "metadata": {
        "id": "WCtz4IJdx1U1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(object):\n",
        "  def __init__(self,  width=28, height=28, channels=1, latent_size = 100):\n",
        "    #initialize variables\n",
        "    self.W = width\n",
        "    self.H = height\n",
        "    self.C = channels\n",
        "    self.OPTIMIZER = Adam(lr=0.0002, decay= 8e-9)\n",
        "\n",
        "    self.LATENT_SPACE_SIZE = latent_size\n",
        "    self.latent_space = np.random.normal(0,1,(self.LATENT_SPACE_SIZE,))\n",
        "\n",
        "    self.Generator = self.model()\n",
        "    self.Generator.compile(loss='binary_crossentropy',optimizer=self.OPTIMIZER)\n",
        "\n",
        "    self.Generator.summary()\n",
        "\n",
        "  def model(self, block_starting_size=128, num_blocks=4):\n",
        "    #Build the generator model and returns it\n",
        "    model = Sequential()\n",
        "\n",
        "    block_size = block_starting_size\n",
        "\n",
        "    model.add(Dense(block_size, input_shape=(self.LATENT_SPACE_SIZE,)))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "    for i in range(num_blocks-1):\n",
        "      block_size = block_size*2\n",
        "      model.add(Dense(block_size))\n",
        "      model.add(LeakyReLU(alpha=0.2))\n",
        "      model.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "    model.add(Dense(self.W*self.H*self.C, activation='tanh'))\n",
        "    model.add(Reshape((self.W, self.H, self.C)))\n",
        "\n",
        "    return model\n",
        "\n",
        "  def summary(self):\n",
        "    #prints the summary of the model to the screen\n",
        "    return self.Generator.summary()\n",
        "\n",
        "  def save_model(self):\n",
        "    #saves the model structure to a file in the data folder\n",
        "    plot_model(self.Discriminator.model, to_file='/content/GAN_100/Generator_model.png')\n"
      ],
      "metadata": {
        "id": "Tk6tntcMy8r7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GAN(object):\n",
        "  def __init__(self, discriminator, generator):\n",
        "    #initialize variables\n",
        "    self.OPTIMIZER = Adam(lr=0.0002, decay=8e-9)\n",
        "    self.Generator = generator\n",
        "    self.Discriminator = discriminator\n",
        "    self.Discriminator.trainable = False\n",
        "    self.gan_model = self.model()\n",
        "    self.gan_model.compile(loss='binary_crossentropy',optimizer=self.OPTIMIZER)\n",
        "    self.gan_model.summary()\n",
        "\n",
        "  def model(self):\n",
        "    #build the adversarial model and return it\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(self.Generator)\n",
        "    model.add(self.Discriminator)\n",
        "\n",
        "    return model\n",
        "\n",
        "  def summary(self):\n",
        "    # prints the model summary to the screen\n",
        "    return self.gan_model.summary()\n",
        "\n",
        "  def save_model(self):\n",
        "    #saves the model structure to a file\n",
        "    plot_model(self.gan_model, to_file='/content/GAN_100/GAN_model.png')\n"
      ],
      "metadata": {
        "id": "D7CHzezy0D85"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "b-dpd8Tk8WWT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "  def __init__(self, width=28, height=28, channels=1, latent_size=100, epochs=50000, batch=32, checkpoint=50, model_type=1):\n",
        "    self.W = width\n",
        "    self.H = height\n",
        "    self.C = channels\n",
        "\n",
        "    self.EPOCHS = epochs\n",
        "    self.BATCH = batch\n",
        "    self.CHECKPOINT =checkpoint\n",
        "    self.model_type = model_type\n",
        "\n",
        "    self.LATENT_SPACE_SIZE = latent_size\n",
        "\n",
        "    self.generator = Generator(height=self.H, width=self.W, channels=self.C, latent_size=self.LATENT_SPACE_SIZE)\n",
        "    self.discriminator = Discriminator(height=self.H, width=self.W, channels=self.C)\n",
        "    self.gan = GAN(generator=self.generator.Generator, discriminator=self.discriminator.Discriminator)\n",
        "\n",
        "    self.load_MNIST()\n",
        "\n",
        "  def load_MNIST(self, model_type=3):\n",
        "    allowed_types = [-1,0,1,2,3,4,5,6,7,8,9]\n",
        "    if self.model_type not in allowed_types:\n",
        "      print('Error: Only Integer values from -1 to 9 are allowed')\n",
        "\n",
        "    (self.X_train, self.Y_train), (_, _) = mnist.load_data()\n",
        "    if self.model_type!=-1:\n",
        "      self.X_train = self.X_train[np.where(self.Y_train==int(self.model_type))[0]]\n",
        "\n",
        "    self.X_train = (np.float32(self.X_train) - 127.5) / 127.5\n",
        "    self.X_train = np.expand_dims(self.X_train, axis=3)\n",
        "    return\n",
        "\n",
        "  def train(self):\n",
        "    for e in range(self.EPOCHS):\n",
        "      #grab a batch\n",
        "      count_real_images = int(self.BATCH/2)\n",
        "      starting_index = np.random.randint(0, (len(self.X_train)-count_real_images))\n",
        "      real_images_raw = self.X_train[starting_index:(starting_index+count_real_images)]\n",
        "      x_real_images = real_images_raw.reshape(count_real_images, self.W, self.H, self.C)\n",
        "      y_real_labels = np.ones([count_real_images,1])\n",
        "\n",
        "      #grab a generated images for this training batch\n",
        "      latent_space_samples = self.sample_latent_space(count_real_images)\n",
        "      x_generated_images = self.generator.Generator.predict(latent_space_samples)\n",
        "      y_generated_labels = np.zeros([self.BATCH-count_real_images,1])\n",
        "\n",
        "      #combine to train on the discriminator\n",
        "      x_batch = np.concatenate([x_real_images, x_generated_images])\n",
        "      y_batch = np.concatenate([y_real_labels, y_generated_labels])\n",
        "\n",
        "      #Now, train the discriminator with this batch\n",
        "      discriminator_loss = self.discriminator.Discriminator.train_on_batch(x_batch,y_batch)[0]\n",
        "\n",
        "      #generate noise\n",
        "      x_latent_space_samples = self.sample_latent_space(self.BATCH)\n",
        "      y_generated_labels = np.ones([self.BATCH,1])\n",
        "      generator_loss = self.gan.gan_model.train_on_batch(x_latent_space_samples, y_generated_labels)\n",
        "\n",
        "      print('Epoch: '+str(int(e))+', [Discriminator :: Loss: '+str(discriminator_loss)+'], [ Generator :: Loss: '+str(generator_loss)+']')\n",
        "      if e % self.CHECKPOINT == 0:\n",
        "        self.plot_checkpoint(e)\n",
        "    return\n",
        "\n",
        "  def sample_latent_space(self, instances):\n",
        "    return np.random.normal(0,1,(instances,self.LATENT_SPACE_SIZE))\n",
        "\n",
        "  def plot_checkpoint(self,e):\n",
        "    filename = '/content/GAN_100/sample_'+str(e)+'.png'\n",
        "    noise = self.sample_latent_space(16)\n",
        "    images = self.generator.Generator.predict(noise)\n",
        "\n",
        "    plt.figure(figsize=(10,10))\n",
        "    for i in range(images.shape[0]):\n",
        "      plt.subplot(4,4,i+1)\n",
        "      image = images[i,:,:,:]\n",
        "      image = np.reshape(image, [self.H, self.W])\n",
        "      plt.imshow(image, cmap='gray')\n",
        "      plt.axis('off')\n",
        "      plt.tight_layout()\n",
        "    plt.savefig(filename)\n",
        "    plt.close('all')\n",
        "    return"
      ],
      "metadata": {
        "id": "QgcocMHD8pYT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HEIGHT = 28\n",
        "WIDTH = 28\n",
        "CHANNEL = 1\n",
        "LATENT_SPACE_SIZE = 100\n",
        "EPOCHS = 40000\n",
        "BATCH = 32\n",
        "CHECKPOINT = 500\n",
        "MODEL_TYPE = -1"
      ],
      "metadata": {
        "id": "LuQ7a9WaE3ix"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(height=HEIGHT, width=WIDTH, channels=CHANNEL, latent_size=LATENT_SPACE_SIZE, epochs=EPOCHS, batch=BATCH,\n",
        "                  checkpoint=CHECKPOINT, model_type=MODEL_TYPE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2SW5aG9E8xC",
        "outputId": "00484cf2-99da-4de0-b2fd-21de8e946ac2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_16 (Dense)            (None, 128)               12928     \n",
            "                                                                 \n",
            " leaky_re_lu_12 (LeakyReLU)  (None, 128)               0         \n",
            "                                                                 \n",
            " batch_normalization_8 (Batc  (None, 128)              512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 256)               33024     \n",
            "                                                                 \n",
            " leaky_re_lu_13 (LeakyReLU)  (None, 256)               0         \n",
            "                                                                 \n",
            " batch_normalization_9 (Batc  (None, 256)              1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 512)               131584    \n",
            "                                                                 \n",
            " leaky_re_lu_14 (LeakyReLU)  (None, 512)               0         \n",
            "                                                                 \n",
            " batch_normalization_10 (Bat  (None, 512)              2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 1024)              525312    \n",
            "                                                                 \n",
            " leaky_re_lu_15 (LeakyReLU)  (None, 1024)              0         \n",
            "                                                                 \n",
            " batch_normalization_11 (Bat  (None, 1024)             4096      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 784)               803600    \n",
            "                                                                 \n",
            " reshape_2 (Reshape)         (None, 28, 28, 1)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,514,128\n",
            "Trainable params: 1,510,288\n",
            "Non-trainable params: 3,840\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " sequential_6 (Sequential)   (None, 28, 28, 1)         1514128   \n",
            "                                                                 \n",
            " sequential_7 (Sequential)   (None, 1)                 923553    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,437,681\n",
            "Trainable params: 1,510,288\n",
            "Non-trainable params: 927,393\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "L90eoNQmFfk2",
        "outputId": "d64d48aa-4fbe-4199-81cc-8a4bad1e63da"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 43ms/step\n",
            "Epoch: 32144, [Discriminator :: Loss: 0.3806874454021454], [ Generator :: Loss: 1.899539589881897]\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "Epoch: 32145, [Discriminator :: Loss: 0.20215913653373718], [ Generator :: Loss: 2.0367908477783203]\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "Epoch: 32146, [Discriminator :: Loss: 0.5031000375747681], [ Generator :: Loss: 1.3483779430389404]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Epoch: 32147, [Discriminator :: Loss: 0.5605377554893494], [ Generator :: Loss: 1.395275592803955]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Epoch: 32148, [Discriminator :: Loss: 0.4069874882698059], [ Generator :: Loss: 1.3901937007904053]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Epoch: 32149, [Discriminator :: Loss: 0.5948561429977417], [ Generator :: Loss: 1.5265759229660034]\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "Epoch: 32150, [Discriminator :: Loss: 0.42391619086265564], [ Generator :: Loss: 1.8759994506835938]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Epoch: 32151, [Discriminator :: Loss: 0.3222929835319519], [ Generator :: Loss: 2.0250778198242188]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Epoch: 32152, [Discriminator :: Loss: 0.32820451259613037], [ Generator :: Loss: 2.3267173767089844]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Epoch: 32153, [Discriminator :: Loss: 0.28307604789733887], [ Generator :: Loss: 2.0048184394836426]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Epoch: 32154, [Discriminator :: Loss: 0.43290314078330994], [ Generator :: Loss: 3.005248546600342]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Epoch: 32155, [Discriminator :: Loss: 0.4668559730052948], [ Generator :: Loss: 1.7721655368804932]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "Epoch: 32156, [Discriminator :: Loss: 0.4882485866546631], [ Generator :: Loss: 1.9799457788467407]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "Epoch: 32157, [Discriminator :: Loss: 0.4313499629497528], [ Generator :: Loss: 1.8073461055755615]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Epoch: 32158, [Discriminator :: Loss: 0.4254642426967621], [ Generator :: Loss: 1.8365139961242676]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "Epoch: 32159, [Discriminator :: Loss: 0.42321306467056274], [ Generator :: Loss: 2.027705669403076]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "Epoch: 32160, [Discriminator :: Loss: 0.3511844277381897], [ Generator :: Loss: 2.5858421325683594]\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "Epoch: 32161, [Discriminator :: Loss: 0.553467333316803], [ Generator :: Loss: 2.4288246631622314]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Epoch: 32162, [Discriminator :: Loss: 0.4833085238933563], [ Generator :: Loss: 2.7067131996154785]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "Epoch: 32163, [Discriminator :: Loss: 0.20115478336811066], [ Generator :: Loss: 2.225280284881592]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Epoch: 32164, [Discriminator :: Loss: 0.39933353662490845], [ Generator :: Loss: 1.9650064706802368]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Epoch: 32165, [Discriminator :: Loss: 0.4462427496910095], [ Generator :: Loss: 2.028261661529541]\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "Epoch: 32166, [Discriminator :: Loss: 0.42284250259399414], [ Generator :: Loss: 2.1558895111083984]\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "Epoch: 32167, [Discriminator :: Loss: 0.44274669885635376], [ Generator :: Loss: 2.3395776748657227]\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "Epoch: 32168, [Discriminator :: Loss: 0.4761021137237549], [ Generator :: Loss: 2.1804213523864746]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Epoch: 32169, [Discriminator :: Loss: 0.35794639587402344], [ Generator :: Loss: 1.890323281288147]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Epoch: 32170, [Discriminator :: Loss: 0.31007617712020874], [ Generator :: Loss: 1.8178348541259766]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Epoch: 32171, [Discriminator :: Loss: 0.2852364182472229], [ Generator :: Loss: 2.0219316482543945]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Epoch: 32172, [Discriminator :: Loss: 0.2971497178077698], [ Generator :: Loss: 1.6705125570297241]\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "Epoch: 32173, [Discriminator :: Loss: 0.35736021399497986], [ Generator :: Loss: 1.7422101497650146]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Epoch: 32174, [Discriminator :: Loss: 0.3865392804145813], [ Generator :: Loss: 1.597222924232483]\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "Epoch: 32175, [Discriminator :: Loss: 0.4149186611175537], [ Generator :: Loss: 2.0859103202819824]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "Epoch: 32176, [Discriminator :: Loss: 0.5151258111000061], [ Generator :: Loss: 1.8432310819625854]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "Epoch: 32177, [Discriminator :: Loss: 0.26462215185165405], [ Generator :: Loss: 2.6471376419067383]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "Epoch: 32178, [Discriminator :: Loss: 0.39646291732788086], [ Generator :: Loss: 2.061707019805908]\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "Epoch: 32179, [Discriminator :: Loss: 0.26267895102500916], [ Generator :: Loss: 1.858750581741333]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Epoch: 32180, [Discriminator :: Loss: 0.21690672636032104], [ Generator :: Loss: 1.6524715423583984]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Epoch: 32181, [Discriminator :: Loss: 0.575284481048584], [ Generator :: Loss: 1.5512299537658691]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Epoch: 32182, [Discriminator :: Loss: 0.34176668524742126], [ Generator :: Loss: 1.7703672647476196]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Epoch: 32183, [Discriminator :: Loss: 0.4504968523979187], [ Generator :: Loss: 2.045421600341797]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-3435b262f1ae>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-70643dedfbc7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m       \u001b[0;31m#grab a generated images for this training batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m       \u001b[0mlatent_space_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_latent_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_real_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m       \u001b[0mx_generated_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_space_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m       \u001b[0my_generated_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBATCH\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mcount_real_images\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2376\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2377\u001b[0m             \u001b[0mbatch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2378\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Single epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2379\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2380\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1305\u001b[0;31m             \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1306\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m       raise RuntimeError(\"`tf.data.Dataset` only supports Python-style \"\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    711\u001b[0m             \u001b[0;34m\"When `dataset` is provided, `element_spec` and `components` must \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m             \"not be specified.\")\n\u001b[0;32m--> 713\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_next_call_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    750\u001b[0m             self._flat_output_types)\n\u001b[1;32m    751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_set_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfulltype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3406\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3407\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3408\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   3409\u001b[0m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[1;32m   3410\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0qXI2oxoGMpf"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}